## NOTE: Ansible has a clustering module for kubernetes we can get to later
# https://docs.ansible.com/ansible/2.9/modules/list_of_clustering_modules.html
---
- name: ADD KUBE CLUSTER ENDPOINT
  hosts: all
  remote_user: root
  gather_facts: no
  vars:
    cluster_admin_hostname: "{{ (groups.admin + groups.lead) | first }}"
    control_plane_endpoint: "{{ hostvars[cluster_admin_hostname].private_ip }}"
  tasks:
    - name: add kube controlplane endpoint alias
      lineinfile:
        path: /etc/hosts
        regexp: 'kube-cluster-endpoint'
        line: '{{ control_plane_endpoint }} kube-cluster-endpoint'



### TODO: Maybe launch the kubernetes cluster services from the first 'lead' server
### nginxProxy (soon-to-be deprecated), buildkitd, gitlab-runner/gitlab-agents
- name: HANDLE KUBERNETES ADMIN
  hosts: "{{ (groups.admin + groups.lead) | first }}"
  remote_user: root
  vars:
    kubernetes_version: "{{ kubernetes_version }}"
    buildkitd_version: "{{ buildkitd_version }}"
    gitlab_tokens: "{{ gitlab_runner_tokens | from_json }}"
    kube_tokens: "'-t {{ gitlab_tokens['service'] }}'"
    vpc_private_iface: "{{ vpc_private_iface }}"
    root_domain_name: "{{ root_domain_name }}"
    admin_servers: "{{ admin_servers }}"
    server_count: "{{ server_count }}"
    active_env_provider: "{{ active_env_provider }}"

    nginx_args: "{{ (admin_servers | int == 0) | ternary('-p default', '') }}"
    runner_img_version: "{{ kubernetes_version | regex_replace('-00') }}"
  tasks:
    - name: init cluster
      block:
        - name: check cluster started
          command: kubectl get nodes
          changed_when: false
      rescue:
        - name: start cluster
          register: initcert
          ## TODO: Pass down helm and skaffold version
          command: bash $HOME/code/scripts/kube/startKubeCluster.sh -v {{ kubernetes_version }} -i {{ vpc_private_iface }} -h 3.8.2-1 -s 2.0.0

    - name: delete old cert certificate-key
      shell: 'kubectl delete secret -n kube-system kubeadm-certs || echo'
      when: (server_count | int > 1) and (initcert is not defined)

    - name: wait
      command: sleep 10

    - name: get certificate-key
      register: certificate_key
      shell: 'kubeadm init phase upload-certs --upload-certs | awk "/^[[:alnum:]]{10,}/"'
      when: server_count | int > 1

    ## Token can expire, update join command.
    #https://stackoverflow.com/questions/61352209/kubernetes-unable-to-join-a-remote-master-node
    - name: create join command
      shell: "kubeadm token create --print-join-command | tee $HOME/.kube/joininfo.txt | consul kv put kube/joincmd -"

    ## Due to various configs/rotations, just untainting the master from get-go
    ##  no mater server count for the time being
    - name: untaint node if solo
      shell: |
          kubectl taint nodes --all node-role.kubernetes.io/control-plane- || echo
          kubectl taint nodes --all node-role.kubernetes.io/master- || echo
      changed_when: false
      #when: server_count | int == 1

    - name: install k8s.core prerequisites
      pip:
        name:
          - kubernetes 
          - pyyaml ## Maybe dont need with "--user" arg now
        extra_args: --user

    ##TODO: Template k8s manifest or use helm instead of script
    ##TODO: Apps change to update deployment
    - name: init nginxKubeProxy
      block:
        - name: check nginxKubeProxy started
          changed_when: false
          k8s:
            api_version: apps/v1
            kind: deployment
            name: nginx-proxy
            namespace: default
      rescue:
        - name: install nginxKubeProxy
          command: bash $HOME/code/scripts/kube/nginxKubeProxy.sh -r {{ root_domain_name }} {{ nginx_args }}

    - name: scale nginx-proxy deployment
      k8s_scale:
        api_version: apps/v1
        kind: deployment
        name: nginx-proxy
        namespace: default
        replicas: "{{ (server_count | int == 1) | ternary(2, server_count) }}" ## Primitive but should work

    - name: create buildkitd namespace
      k8s:
        api_version: v1
        name: buildkitd
        kind: namespace
        state: present

    ## Move to below CSI init if we start to use volumes
    ## TODO: Eventually configure and deploy rootless version
    - name: init buildkitd
      block:
        - name: check buildkitd started
          changed_when: false
          k8s:
            api_version: apps/v1
            kind: statefulset
            name: buildkitd
            namespace: buildkitd
      rescue:
        - name: copy buildkitd template to node
          template:
            src: ansiblefiles/buildkitd.yaml
            dest: /tmp/buildkitd.yaml
        - name: install buildkitd
          k8s:
            state: present
            namespace: buildkitd
            src: /tmp/buildkitd.yaml

    ##TODO: Check if exists before launching or pay attention to current scale
    - name: init minioKubeAzGateway
      command: bash $HOME/code/scripts/kube/minioKubeAzGateway.sh
      when: "'azure' in active_env_provider"

    - name: cluster serviceaccounts
      block:
        - name: check accounts added
          shell: "gitlab-runner verify --delete && gitlab-runner -log-format json list 2>&1 >/dev/null | grep 'deploy-kube-runner'"
          changed_when: false
      rescue:
        - name: create cluster serviceaccounts
          command: bash $HOME/code/scripts/kube/createClusterAccounts.sh -v {{ runner_img_version }} -d {{ root_domain_name }} \
            -a {{ private_ip }} -l {{ root_domain_name }} -b buildkitd-0 {{ (gitlab_tokens['service'] != '') | ternary(kube_tokens, '') }} -u -o
      when: admin_servers | int > 0

    - name: add agents to gitlab
      block:
        - name: check agents added
          command: consul kv get kube/gitlab_integrated
          changed_when: false
      rescue:
        - name: add agents
          command: bash $HOME/code/scripts/kube/addAgentsToGitlab.sh -d {{ root_domain_name }} -u
      when: admin_servers | int > 0


- name: HANDLE KUBERNETES CONTROL-PLANE
  ### Get lead hosts that do NOT match kubernetes admin
  ### (lead) NOT IN (admin+lead | first)
  hosts: "{{ (groups.lead) | difference( ((groups.admin + groups.lead) | first) ) }}"
  remote_user: root
  gather_facts: no
  vars:
    kubernetes_version: "{{ kubernetes_version }}"
    cluster_admin_hostname: "{{ (groups.admin + groups.lead) | first }}"
    certificate_key: "{{ hostvars[cluster_admin_hostname]['certificate_key'].stdout }}"
  tasks:
    - name: join cluster as control_plan
      block:
        - name: check cluster joined
          command: kubectl get nodes
          changed_when: false
      rescue:
        - name: join cluster
          ##TODO: Pass down helm and skaffold version
          shell: "bash $HOME/code/scripts/kube/joinKubeCluster.sh -v {{ kubernetes_version }} -k {{ certificate_key }} -h 3.8.2-1 -s 2.0.0"

    - name: untaint secondary control plane
      shell: "kubectl taint nodes {{ machine_name }} node-role.kubernetes.io/master- || echo"
      changed_when: false


- name: Handle Kubernetes worker
  ### Get all hosts that do NOT match kubernetes admin/control-plane
  ### (db+build) NOT IN (admin+lead)
  hosts: "{{ (groups.db + groups.build) | difference(groups.admin + groups.lead) }}"
  remote_user: root
  gather_facts: no
  vars:
    kubernetes_version: "{{ kubernetes_version }}"
  tasks:
    - name: join cluster
      block:
        - name: check cluster joined
          command: kubectl get nodes
          changed_when: false
      rescue:
        - name: join cluster
          ##TODO: Pass down helm and skaffold version
          shell: "bash $HOME/code/scripts/kube/joinKubeCluster.sh -v {{ kubernetes_version }} -h 3.8.2-1 -s 2.0.0"


# TODO: Implement csi config for aws then azure
# TODO: Automated cleanup of volumes in intelligent way
- name: START KUBERNETES CLOUD CSI
  hosts: "{{ (groups.admin + groups.lead) | first }}"
  remote_user: root
  gather_facts: no
  vars:
    cloud_provider: "{{ cloud_provider }}"
    cloud_provider_token: "{{ cloud_provider_token }}"
    csi_namespace: "{{ csi_namespace }}"
    csi_version: "{{ csi_version }}"
    snapshot_secret_name: "snapshot-validation-secret"
    ca_bundle: ""
  tasks:
    - name: ensure digitalocean csi secret present
      block:
        - name: check digitalocean csi secret
          command: 'kubectl get secret -n {{ csi_namespace }} {{ cloud_provider }}'
          changed_when: false
      rescue:
        - name: add digitalocean csi secret file
          template:
            src: ansiblefiles/csi-secret.yaml
            dest: /root/.kube/csi-secret.yaml

        - name: apply digitalocean csi secret
          shell: 'kubectl apply -f /root/.kube/csi-secret.yaml'

        - name: rm digitalocean csi secret file
          file: 
            path: /root/.kube/csi-secret.yaml
            state: absent
      when: cloud_provider == "digitalocean" and cloud_provider_token != ""

    # TODO: Do this using cert-manager
    - name: ensure snapshot validation secret present
      block:
        - name: check snapshot validation secret
          command: 'kubectl get secret -n {{ csi_namespace }} {{ snapshot_secret_name }}'
          changed_when: false
      rescue:
        - name: add digitalocean create-cert script
          template:
            src: ansiblefiles/create-cert.sh
            dest: /tmp/create-cert.sh

        - name: chmod cert script
          command: "chmod +x /tmp/create-cert.sh"

        - name: create key/cert and populate secret
          shell: '/tmp/create-cert.sh --service snapshot-validation-service --secret {{ snapshot_secret_name }} --namespace {{ csi_namespace }}'
      when: cloud_provider == "digitalocean" and cloud_provider_token != ""

    - name: ensure snapshot validation deployed
      block:
        - name: check snapshot validation deployment
          command: kubectl rollout status -w "deployment/snapshot-validation" -n {{ csi_namespace }}
          changed_when: false
      rescue:
        - name: populate ca_bundle for snapshot webhook
          register: ca_bundle
          shell: |
            kubectl config view --raw -o json | jq -r '.clusters[0].cluster."certificate-authority-data"' | tr -d '"';

        - name: add snapshot webhook config
          template:
            src: ansiblefiles/{{ csi_version }}-snapshot-validation-webhook.yaml
            dest: /root/.kube/{{ csi_version }}-snapshot-validation-webhook.yaml

        - name: apply digitalocean snapshot webhook config
          shell: |
            kubectl apply -f /root/.kube/{{ csi_version }}-snapshot-validation-webhook.yaml;
            kubectl rollout status -w "deployment/snapshot-validation" -n {{ csi_namespace }};

        - name: rm digitalocean webhook file
          file: 
            path: /root/.kube/{{ csi_version }}-snapshot-validation-webhook.yaml
            state: absent
      when: cloud_provider == "digitalocean" and cloud_provider_token != ""

    # Apply multiple times (dependencies take a second to fully apply)
    # We're cheesing it and looping 3 times with 3 second delay for first iteration of this
    - name: apply digitalocean csi config
      shell: |
        # Do *not* add a blank space after -f
        kubectl apply -fhttps://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-v{{ csi_version }}/crds.yaml;
        kubectl apply -fhttps://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-v{{ csi_version }}/driver.yaml;
        kubectl apply -fhttps://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-v{{ csi_version }}/snapshot-controller.yaml;
        sleep 3;
      with_sequence: count=3
      when: cloud_provider == "digitalocean" and cloud_provider_token != ""


- name: REBOOT GITLAB ENVIRONMENTS
  hosts: "{{ (groups.admin | length > 0) | ternary( (groups.admin|first), [] ) }}"
  remote_user: root
  vars:
    root_domain_name: "{{ root_domain_name }}"
    import_gitlab: "{{ import_gitlab }}"
  tasks:
    - name: check cluster
      block:
        - name: check cluster started
          command: kubectl get nodes
          changed_when: false
      rescue:
        - name: inform error
          shell: "echo 'Cluster not started, which means something went wrong'; exit 1;"

    - name: init envs
      block:
        - name: check envs rebooted
          command: consul kv get kube/envs_rebooted
          changed_when: false
      rescue:
        - name: reboot envs
          shell: |
            FILENAME=ENVS.txt
            SNIPPET_PROJECT_ID=7
            SNIPPET_ID=33
            LOCAL_FILE="$HOME/code/backups/$FILENAME"
            DEFAULT_BRANCH="master"

            ## Download list of production environments
            curl "https://gitlab.{{ root_domain_name }}/api/v4/projects/$SNIPPET_PROJECT_ID/snippets/$SNIPPET_ID/files/main/$FILENAME/raw" > $LOCAL_FILE
            ## Alternative without api
            ## curl -sL "https://gitlab.{{ root_domain_name }}/os/workbench/-/snippets/$SNIPPET_ID/raw/main/$FILENAME -o $LOCAL_FILE"

            ## Gen tmp TOKEN to trigger deploy_prod job in each project listed
            TERRA_UUID=$(uuidgen)
            sudo gitlab-rails runner "token = User.find(1).personal_access_tokens.create(scopes: [:api], name: 'Temp PAT'); token.set_token('$TERRA_UUID'); token.save!";

            ## Iterate over projects in $LOCAL_FILE and run pipeline for each
            while read PROJECT_ID; do
                echo $PROJECT_ID;

                ## Create trigger and get [token, id]
                TRIGGER_INFO=$(curl -X POST -H "PRIVATE-TOKEN: $TERRA_UUID" --form description="reboot" \
                    "https://gitlab.{{ root_domain_name }}/api/v4/projects/$PROJECT_ID/triggers")

                TRIGGER_TOKEN=$(echo $TRIGGER_INFO | jq -r '.token')
                TRIGGER_ID=$(echo $TRIGGER_INFO | jq -r '.id')

                ## Trigger pipeline
                curl -X POST --form "variables[ONLY_DEPLOY_PROD]=true" \
                "https://gitlab.{{ root_domain_name }}/api/v4/projects/$PROJECT_ID/trigger/pipeline?token=$TRIGGER_TOKEN&ref=$DEFAULT_BRANCH"

                ## Delete trigger
                curl -X DELETE -H "PRIVATE-TOKEN: $TERRA_UUID" "https://gitlab.{{ root_domain_name }}/api/v4/projects/$PROJECT_ID/triggers/$TRIGGER_ID";

            done <$LOCAL_FILE

            ## Revoke token
            sudo gitlab-rails runner "PersonalAccessToken.find_by_token('$TERRA_UUID').revoke!";
            consul kv put kube/envs_rebooted true
      when: import_gitlab | bool == true


- name: UPDATE COREDNS
  hosts: "{{ (groups.admin + groups.lead) | first }}"
  remote_user: root
  vars:
    root_domain_name: "{{ root_domain_name }}"
    configmap_keys: []
  tasks:
    - name: get patchfiles
      register: patchfiles
      find:
        paths: /root/.kube
        patterns: "*-cm-patch.yml"

    - name: add hosts to coredns configmap
      shell: 'kubectl patch cm -n kube-system coredns --type merge --patch "$(cat {{ item.path }})"'
      loop: "{{ patchfiles.files }}"
      when: patchfiles.matched > 0

    - name: get configmap keys from file
      register: configmap_patch_keys
      command: 'sed -n -r "s/ +([[:alnum:]]+): \|/\1/p" {{ item.path }}'
      loop: "{{ patchfiles.files }}"
      when: patchfiles.matched > 0

    - name: make configmap_keys
      set_fact:
        configmap_keys: "{{ configmap_keys + item.stdout_lines }}"
      loop: '{{ configmap_patch_keys.results | list }}'

      ### Might have to do hosts HOSTNAME { _hosts_ fallthrough } in cm's
      ###  if we have multiple hosts directives for the same host
    - name: add hosts directive to coredns configmap
      register: update
      shell: |
        kubectl get cm -n kube-system coredns -o yaml | sed "/hosts \/etc\/coredns\/{{ item }}/d" \
            | sed -r "s|( +)loadbalance|\1loadbalance\n\1hosts /etc/coredns/{{ item }} {{ root_domain_name }}|" \
            | kubectl replace -f -
        sleep 1
      loop: "{{ configmap_keys }}"

    - name: update coredns deployment volume file
      template:
        src: ansiblefiles/coredns-vol-patch.yml
        dest: /root/.kube/coredns-vol-patch.yml

    ## Only redeploys the deployment on volume change
    ## CoreDNS reads its Corefile in its configmap periodically, updating any changes gracefully
    - name: patch coredns deployment
      shell: 'kubectl patch deployment -n kube-system coredns --type merge --patch "$(cat /root/.kube/coredns-vol-patch.yml)"'
      ignore_errors: true


- name: UPDATE KUBECONFIG
  ## For now just the main config - so we dont clobber if merging locally
  hosts: "{{ (groups.admin + groups.lead) | first }}"
  remote_user: root
  vars:
    root_domain_name: "{{ root_domain_name }}"
  tasks:
    - name: update kubeconfig values
      shell: |
        sed -r -i "s|kubernetes-admin@kubernetes|{{ root_domain_name }}|" $HOME/.kube/config
        sed -r -i "s|kubernetes|{{ root_domain_name }}|" $HOME/.kube/config
      
    - name: ansible kubeconfig file from remote to local.
      fetch:
        src: ~/.kube/config
        dest: ~/.kube/{{ root_domain_name }}-kubeconfig
        flat: yes
